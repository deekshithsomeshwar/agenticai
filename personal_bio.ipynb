{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95361b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52a68cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "436e8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8006604f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key set\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\") \n",
    "else:\n",
    "    print(\"API key set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e27aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read PDF and extract text\n",
    "reader = PdfReader(\"info folder/profile.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3534d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the summary.rtf file\n",
    "with open(\"info folder/summary.rtf\", \"r\") as file:\n",
    "    summary = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5fef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2821\n",
      "\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n",
      "{\\colortbl;\\red255\\green255\\blue255;}\n",
      "{\\*\\expandedcolortbl;;}\n",
      "\\paperw11900\\paperh16840\\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n",
      "\\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n",
      "\n",
      "\\f0\\fs24 \\cf0 \\\n",
      "\\\n",
      "### **Education**\\\n",
      "\\\n",
      "* **Schooling (1st\\'9610th Standard):**\\\n",
      "  Attended **Oxford English School**, Bangalore.\\\n",
      "  Secured **88%** in the 10th standard (SSLC), which is considered a **distinction**.\\\n",
      "\\\n",
      "* **Pre-University (PU College):**\\\n",
      "  Studied at **The Oxford PU College**, Bangalore.\\\n",
      "  Graduated with **66%**, categorized as a **second class**.\\\n",
      "\\\n",
      "* **Undergraduate Degree:**\\\n",
      "  Completed a **Bachelor of Arts (B.A.)** specializing in **Psychology** from **University of Mysore**, Mysore.\\\n",
      "  The degree was pursued through **distance education / correspondence mode**.\\\n",
      "\\\n",
      "* **Postgraduate Education:**\\\n",
      "  **Currently pursuing an MBA** (Master of Business Administration) with a specialization in **Information Technology (IT)** from **University of Mysore**, Mysore.\\\n",
      "  This program is also being completed via **distance education / correspondence**.\\\n",
      "\\\n",
      "---\\\n",
      "\\\n",
      "###  **Work Experience**\\\n",
      "\\\n",
      "* Brings **20 years of experience** in the **Information Technology (IT)** industry.\\\n",
      "* Specialized in **Project Management** and **Program Management**, with significant leadership in delivering large-scale technology initiatives.\\\n",
      "* Strong focus on **Agile methodologies**, particularly **Scrum** and **Kanban**, to drive high-performance team outcomes.\\\n",
      "* Worked **primarily in product-based companies**, including **Dell** and **HP**, gaining deep exposure to enterprise product development, cross-functional collaboration, and customer-centric innovation.\\\n",
      "* Experienced in aligning delivery execution with business goals and navigating dynamic tech landscapes.\\\n",
      "\\\n",
      "---\\\n",
      "\\\n",
      "###  **Interests & Learning**\\\n",
      "\\\n",
      "* Passionate about **continuous learning** and staying updated with **emerging technologies**.\\\n",
      "* Currently inclined toward exploring and understanding new trends in **Artificial Intelligence (AI)** and **adapting to evolving market demands**.\\\n",
      "* Interested in leveraging AI tools and frameworks to improve delivery, automation, and decision-making in IT programs.\\\n",
      "\\\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#check if the summary is read correctly\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9b9cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the system prompt\n",
    "name = \"Deekshith Someshwar\"\n",
    "system_prompt = f\"You are acting as {name}. You are going to answer question on {name}'s personal bio. \\\n",
    "You will be given a summary of {name}'s personal bio and a LinkedIn profile. You will answer the question \\\n",
    "based on the summary and LinkedIn profile. Your responsibility is to answer questions about {name} \\\n",
    "as professionally and accurately as possible. You are given {name}'s summary and linkedin profile, and its  \\\n",
    "your responsibility to answer questions about {name} as professionally and accurately as possible. \\\n",
    "You will be given a question and you will answer the question based on the summary and LinkedIn profile. \\\n",
    "You will politely decline to answer {name}'s personal information other than what is provided and mention this is a professional \\\n",
    "bio and to reach out to me if they wish to know me personally\"\n",
    "system_prompt += f\"\\n\\nSummary:\\n{summary}\\n\\nLinkedIn Profile:\\n{linkedin}\"    \n",
    "system_prompt += \"\\n\\nYou will be given a question and you will answer the question based on the summary and LinkedIn profile.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8da47d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b26ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradio chat interface\n",
    "gr.ChatInterface(chat, type= \"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5aa3ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a pydantic model to validate the output and come up with better output in case of errors\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80dd72d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e30303c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator. You will evaluate the output of the chat interface and decide if \\\n",
    "the output is acceptable or not. You will also provide feedback on the output. \\\n",
    "The agent is playing the role of {name} and is given to responsibility to answer questions about {name}'s personal bio. \\\n",
    "You will be given the output of the chat interface and you will evaluate the output based on the following criteria: \\\n",
    "1. The output should be relevant to the question asked. \\\n",
    "2. The output should be professional and accurate. \\\n",
    "3. The output should not contain any personal information other than what is provided in the summary and LinkedIn profile. \\\n",
    "4. The output should be polite and respectful. \\\n",
    "5. The output should not contain any offensive or inappropriate content. \"\n",
    "evaluator_system_prompt += f\"\\n\\n you will be given the {summary} and {linkedin} to evaluate the output. \"\n",
    "evaluator_system_prompt += \"\\n\\n With this context, you need to decide if the output is acceptable or not, and suggest to make changes \\\n",
    "    if it is not acceptable. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b6ed0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\" Here is the output of the chat interface:\\n\\n{reply}\\n\\nQuestion: {message}\\n\\nHistory: {history}\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15169aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "    response = openai.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        response_format = Evaluation\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c0c59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + f\"\\n\\nResponse rejected by the evaluator: {feedback}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": updated_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )   \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "85960f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "        ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Output is acceptable\" )\n",
    "    else:\n",
    "        print(\"Output is not acceptable. Feedback: \", evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a5ec033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/deekshithsomeshwar/Documents/projects/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/hk/0p7x9sxj6vxd53kxmv83htgh0000gn/T/ipykernel_1574/274929641.py\", line 13, in chat\n",
      "    if evaluation.is_acceptable:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'str' object has no attribute 'is_acceptable'\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bced1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
